#histogram of mean education with 8 breaks with defined ranges
ids <- c("<2", "2~5", "5~8", "8~10", "10-13", "13-15", "15~19", ">=19")
brks <- c(-1, 2, 5, 8, 10, 13, 15, 19, 100)
bins <- cut(income$MeanEducation, breaks = brks, include.lowest = TRUE, labels = ids)
plot(bins)
#create a new ordered vector named eGroup
eGroup<- vector(mode = "character", length = length(income$MeanEducation))
eGroup[income$MeanEducation<2] <-"<2)"
eGroup[income$MeanEducation>=2 & income$MeanEducation<5] <-"2-5)"
eGroup[income$MeanEducation>=5 & income$MeanEducation<8] <-"5-8)"
eGroup[income$MeanEducation>=8 & income$MeanEducation<10] <-"8-10)"
eGroup[income$MeanEducation>=10 & income$MeanEducation<13] <-"10-13)"
eGroup[income$MeanEducation>=13 & income$MeanEducation<15] <-"13-15)"
eGroup[income$MeanEducation>=15 & income$MeanEducation<19] <-"15-19)"
eGroup[income$MeanEducation >=19] <-">19"
eGroup <- factor(eGroup, levels = c("<2)", "2-5)", "5-8)", "8-10)", "10-13)", "13-15)", "15-19)", ">19"),
ordered = TRUE)
income <- cbind(income, eGroup)
str(income$eGroup)
head(income$eGroup)
summary(eGroup)
#plot the egroup vs income
plot(income$eGroup, income$MeanHouseholdIncome)
#create a subset and plot the new income
zip <- income$Zip1[income$MeanHouseholdIncome > 7000 & income$MeanHouseholdIncome < 200000]
realIncome <- income$MeanHouseholdIncome[income$MeanHouseholdIncome > 7000 & income$MeanHouseholdIncome < 200000]
boxplot(realIncome~zip, ylab = "Income", xlab = "Zip")+
title("Average Household Income by Zip Code")
#using the same subsets make a boxplot with a log of the y
boxplot(realIncome~zip, ylab = "Income", xlab = "Zip", log = "y")+
title("Average Household Income by Zip Code")
#make a plot using ggplot2 as with x of the the var realZip and var realIncome
library(ggplot2)
library(proto)
DF <- as.data.frame((cbind(realIncome, zip)))
ggplot(DF, aes(x =zip, y = realIncome))+
geom_point(alpha = 0.2, position = "jitter") +
scale_y_log10()
#add color and boxes to the ggplot
ggplot(data=DF, aes(x=as.factor(zip), y=realIncome)) +
geom_point(aes(colour=factor(zip)), position="jitter", alpha=0.2) +
geom_boxplot(alpha=0.1, outlier.size=0) +
scale_y_log10() + ylab("Income") + xlab("Zip Code") +
ggtitle("Average Income by Zip Code") +
theme(plot.title = element_text(lineheight=.8, face="bold"))
#do a gg plot for another two comparisons: egroup vs income and education vs zip
egroup <- income$eGroup[income$MeanHouseholdIncome > 7000 & income$MeanHouseholdIncome < 200000]
education <- income$MeanEducation[income$MeanHouseholdIncome > 7000 & income$MeanHouseholdIncome < 200000]
DF <- as.data.frame(cbind(DF, egroup, education))
#egroup vs income
ggplot(data=DF, aes(x=as.factor(egroup), y=realIncome)) +
geom_point(aes(colour=factor(egroup)), position="jitter", alpha=0.2) +
geom_boxplot(alpha=0.1, outlier.size=0) +
scale_y_log10() + ylab("Income") + xlab("Education Groups") +
ggtitle("Average Income by Zip Code") +
theme(plot.title = element_text(lineheight=.8, face="bold"))
#education vs zip
ggplot(data=DF, aes(x=as.factor(zip), y=education)) +
geom_point(aes(colour=factor(zip)), position="jitter", alpha=0.2) +
geom_boxplot(alpha=0.1, outlier.size=0) +
scale_y_log10() + ylab("Income") + xlab("Zip Code") +
ggtitle("Average Income by Zip Code") +
theme(plot.title = element_text(lineheight=.8, face="bold"))
ggplot(data=DF, aes(x=as.factor(egroup), y=realIncome)) +
geom_point(aes(colour=factor(egroup)), position="jitter", alpha=0.2) +
geom_boxplot(alpha=0.1, outlier.size=0)+
ylab("Income") + xlab("Education Groups") +
ggtitle("Average Income by Education Group") +
theme(plot.title = element_text(lineheight=.8, face="bold"))
ggplot(data=DF, aes(x=as.factor(egroup), y=realIncome)) +
geom_point(aes(colour=factor(egroup)), position="jitter", alpha=0.2) +
geom_boxplot(alpha=0.1, outlier.size=0)+
scale_y_log10()+ylab("Income") + xlab("Education Groups") +
ggtitle("Average Income by Education Group") +
theme(plot.title = element_text(lineheight=.8, face="bold"))
ggplot(data=DF, aes(x=as.factor(zip), y=education)) +
geom_point(aes(colour=factor(zip)), position="jitter", alpha=0.2) +
geom_boxplot(alpha=0.1, outlier.size=0)+
ylab("Education") + xlab("Zip Code") +
ggtitle("Average Education by Zip Code") +
theme(plot.title = element_text(lineheight=.8, face="bold"))
theme(plot.title = element_text(lineheight=.8, face="bold"))
summary(realIncome)
shiny::runApp('App-1/census-app/stockVis/twitteRGUI')
shiny::runApp('App-1/census-app/stockVis/twitteRGUI')
shiny::runApp('App-1/census-app')
load("C:/Users/Richard/Downloads/twitterSearchAPI/my_oauth.Rdata")
load("~/App-1/census-app/stockVis/twitteRGUI/my_oauth.Rdata")
load("~/App-1/census-app/stockVis/twitteRGUI/my_oauth.Rdata")
setwd("~/shinyTwitterWordcloud")
shiny::runApp()
shiny::runApp()
shiny::runApp()
library(stringr)
library(base64enc)
library(bitops)
library(RCurl)
library(RJSONIO)
library(ROAuth)
library(twitteR)
library(stringr)
library(tm)
library(NLP)
library(ggplot2)
library(wordcloud)
library(SnowballC)
library(RColorBrewer)
#~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~
# Part 1: Collect tweets using Search API
#~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~
api_key <- 'sjjI4w33PqZz6AR2noLKPziqT'
api_secret <- 'ZmzBPig0L3KHEVEnS4PZ2PAwy0UsZNttDXTwqCfxhR2fBkZFqt'
token <- '2419123201-mv8GCAvaN3TQ6IgODq1XNyMbZIUxqO7Fc5WecG2'
token_secret <- 'YCKkImPYyrseusHbCEebPuvmsOf00M5f9dIiWE5N6y3fU'
setup_twitter_oauth(api_key, api_secret, token, token_secret)
# sample searchTwitter, you can replace this with other keywords
tweets <- searchTwitter("'IUN' OR 'IUNorthwest'", n=500, lang="en")
tweets.df <- twListToDF(tweets)
# extract text and clean text
tweets <- tweets.df$text
#~~~~~~~~~~~~~~~~~~~~~~~
# Part 2: Clean tweets
#~~~~~~~~~~~~~~~~~~~~~~~~~~~~
# Remove redundant spaces from tweets
tweets <- str_replace_all(tweets, "  ", " ")
# Replace new lines with white spaces
tweets <- str_replace_all(tweets, "\n", " ")
# Remove URLs
tweets <- str_replace_all(tweets, "http[^[:space:]]*", "")
# Remove retweet header (RT @screenname)
tweets <- str_replace_all(tweets, "RT\\s@[a-zA-Z0-9_]*:\\s","")
# Remove hashtags
# tweets <- str_replace_all(tweets, "#[a-zA-Z]*","")
# Remove references to other screennames
tweets <- str_replace_all(tweets, "@[a-zA-Z0-9_]*","")
tweets.clean <- tweets
#~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~
# Part 3: Build a corpus 'tweetCorpus' on tweets.clean
# Transform the documents within a corpus
# help documents: R-Pakckage-tm.pdf
#~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~
library(tm)
#Build a corpus 'tweetCorpus' on tweets.clean
tweetsCorpus<- VCorpus(VectorSource(tweets.clean))
#~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~
# Part 4: Transform the documents in tweetCorpus
# tm_map: interface to apply transformation functions to corpora.
#~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~
# transform to lowercases
tweetsCorpus <- tm_map(tweetsCorpus, content_transformer(tolower))
# remove punctuations
tweetsCorpus <- tm_map(tweetsCorpus, removePunctuation)
# remove whitespaces
tweetsCorpus <- tm_map(tweetsCorpus, stripWhitespace)
# remove stopwords
tweetsCorpus <- tm_map(tweetsCorpus, removeWords, stopwords("english"))
#~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~
# Part 5: Create a Document-Term Matrix over tweetCorpus
#~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~
# Create a Document-Term matrix 'tweetsDTM'
tweetDTM <- DocumentTermMatrix(tweetsCorpus)
# Find terms that occur at least ten times
findFreqTerms(tweetDTM, lowfreq=10)
# The total frequency of each terms across the corpus
freq <- colSums(as.matrix(tweetDTM))
freq.order <- order(freq, decreasing = TRUE)
freq[head(freq.order)] #most frequent terms
freq[tail(freq.order)] #least frequent terms
# Question: Which group is more interesting, most frequent terms or least frequent terms?
#more
#~~~~~~~~~~~~~~~~~~~~~~
# Find Associated Terms with a specific term, say 'food', with a minimum correlation 0.4
#~~~~~~~~~~~~~~~~~~~~~~~~
findAssocs(tweetDTM, "food", 0.4)
#~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~
# Part 6: Creat a word cloud from tweetCorpus using weighted TFIDF
# export the cloud to a PNG file
#~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~
library(wordcloud)
wordcloud(weightTf(tweetDTM$dimnames$Terms), freq)
tweets <- searchTwitter("Softball", n=1000, lang="en")
library(stringr)
library(base64enc)
library(bitops)
library(RCurl)
library(RJSONIO)
library(ROAuth)
library(twitteR)
library(stringr)
library(tm)
library(NLP)
library(ggplot2)
library(wordcloud)
library(SnowballC)
library(RColorBrewer)
#~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~
# Part 1: Collect tweets using Search API
#~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~
api_key <- 'sjjI4w33PqZz6AR2noLKPziqT'
api_secret <- 'ZmzBPig0L3KHEVEnS4PZ2PAwy0UsZNttDXTwqCfxhR2fBkZFqt'
token <- '2419123201-mv8GCAvaN3TQ6IgODq1XNyMbZIUxqO7Fc5WecG2'
token_secret <- 'YCKkImPYyrseusHbCEebPuvmsOf00M5f9dIiWE5N6y3fU'
setup_twitter_oauth(api_key, api_secret, token, token_secret)
# sample searchTwitter, you can replace this with other keywords
tweets <- searchTwitter("Softball", n=1000, lang="en")
tweets.df <- twListToDF(tweets)
# extract text and clean text
tweets <- tweets.df$text
#~~~~~~~~~~~~~~~~~~~~~~~
# Part 2: Clean tweets
#~~~~~~~~~~~~~~~~~~~~~~~~~~~~
# Remove redundant spaces from tweets
tweets <- str_replace_all(tweets, "  ", " ")
# Replace new lines with white spaces
tweets <- str_replace_all(tweets, "\n", " ")
# Remove URLs
tweets <- str_replace_all(tweets, "http[^[:space:]]*", "")
# Remove retweet header (RT @screenname)
tweets <- str_replace_all(tweets, "RT\\s@[a-zA-Z0-9_]*:\\s","")
# Remove hashtags
# tweets <- str_replace_all(tweets, "#[a-zA-Z]*","")
# Remove references to other screennames
tweets <- str_replace_all(tweets, "@[a-zA-Z0-9_]*","")
tweets.clean <- tweets
#~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~
# Part 3: Build a corpus 'tweetCorpus' on tweets.clean
# Transform the documents within a corpus
# help documents: R-Pakckage-tm.pdf
#~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~
library(tm)
#Build a corpus 'tweetCorpus' on tweets.clean
tweetsCorpus<- VCorpus(VectorSource(tweets.clean))
#~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~
# Part 4: Transform the documents in tweetCorpus
# tm_map: interface to apply transformation functions to corpora.
#~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~
# transform to lowercases
tweetsCorpus <- tm_map(tweetsCorpus, content_transformer(tolower))
# remove punctuations
tweetsCorpus <- tm_map(tweetsCorpus, removePunctuation)
# remove whitespaces
tweetsCorpus <- tm_map(tweetsCorpus, stripWhitespace)
# remove stopwords
tweetsCorpus <- tm_map(tweetsCorpus, removeWords, stopwords("english"))
#~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~
# Part 5: Create a Document-Term Matrix over tweetCorpus
#~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~
# Create a Document-Term matrix 'tweetsDTM'
tweetDTM <- DocumentTermMatrix(tweetsCorpus)
# Find terms that occur at least ten times
findFreqTerms(tweetDTM, lowfreq=10)
# The total frequency of each terms across the corpus
freq <- colSums(as.matrix(tweetDTM))
freq.order <- order(freq, decreasing = TRUE)
freq[head(freq.order)] #most frequent terms
freq[tail(freq.order)] #least frequent terms
# Question: Which group is more interesting, most frequent terms or least frequent terms?
#more
#~~~~~~~~~~~~~~~~~~~~~~
# Find Associated Terms with a specific term, say 'food', with a minimum correlation 0.4
#~~~~~~~~~~~~~~~~~~~~~~~~
findAssocs(tweetDTM, "food", 0.4)
#~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~
# Part 6: Creat a word cloud from tweetCorpus using weighted TFIDF
# export the cloud to a PNG file
#~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~
library(wordcloud)
wordcloud(weightTf(tweetDTM$dimnames$Terms), freq)
library(stringr)
library(base64enc)
library(bitops)
library(RCurl)
library(RJSONIO)
library(ROAuth)
library(twitteR)
library(stringr)
library(tm)
library(NLP)
library(ggplot2)
library(wordcloud)
library(SnowballC)
library(RColorBrewer)
#~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~
# Part 1: Collect tweets using Search API
#~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~
api_key <- 'sjjI4w33PqZz6AR2noLKPziqT'
api_secret <- 'ZmzBPig0L3KHEVEnS4PZ2PAwy0UsZNttDXTwqCfxhR2fBkZFqt'
token <- '2419123201-mv8GCAvaN3TQ6IgODq1XNyMbZIUxqO7Fc5WecG2'
token_secret <- 'YCKkImPYyrseusHbCEebPuvmsOf00M5f9dIiWE5N6y3fU'
setup_twitter_oauth(api_key, api_secret, token, token_secret)
# sample searchTwitter, you can replace this with other keywords
tweets <- searchTwitter("Softball", n=1000, lang="en")
tweets.df <- twListToDF(tweets)
# extract text and clean text
tweets <- tweets.df$text
#~~~~~~~~~~~~~~~~~~~~~~~
# Part 2: Clean tweets
#~~~~~~~~~~~~~~~~~~~~~~~~~~~~
# Remove redundant spaces from tweets
tweets <- str_replace_all(tweets, "  ", " ")
# Replace new lines with white spaces
tweets <- str_replace_all(tweets, "\n", " ")
# Remove URLs
tweets <- str_replace_all(tweets, "http[^[:space:]]*", "")
# Remove retweet header (RT @screenname)
tweets <- str_replace_all(tweets, "RT\\s@[a-zA-Z0-9_]*:\\s","")
# Remove hashtags
# tweets <- str_replace_all(tweets, "#[a-zA-Z]*","")
# Remove references to other screennames
tweets <- str_replace_all(tweets, "@[a-zA-Z0-9_]*","")
tweets.clean <- tweets
#~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~
# Part 3: Build a corpus 'tweetCorpus' on tweets.clean
# Transform the documents within a corpus
# help documents: R-Pakckage-tm.pdf
#~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~
library(tm)
#Build a corpus 'tweetCorpus' on tweets.clean
tweetsCorpus<- VCorpus(VectorSource(tweets.clean))
#~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~
# Part 4: Transform the documents in tweetCorpus
# tm_map: interface to apply transformation functions to corpora.
#~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~
# transform to lowercases
tweetsCorpus <- tm_map(tweetsCorpus, content_transformer(tolower))
# remove punctuations
tweetsCorpus <- tm_map(tweetsCorpus, removePunctuation)
# remove whitespaces
tweetsCorpus <- tm_map(tweetsCorpus, stripWhitespace)
# remove stopwords
tweetsCorpus <- tm_map(tweetsCorpus, removeWords, stopwords("english"))
#~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~
# Part 5: Create a Document-Term Matrix over tweetCorpus
#~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~
# Create a Document-Term matrix 'tweetsDTM'
tweetDTM <- DocumentTermMatrix(tweetsCorpus)
# Find terms that occur at least ten times
findFreqTerms(tweetDTM, lowfreq=10)
# The total frequency of each terms across the corpus
freq <- colSums(as.matrix(tweetDTM))
freq.order <- order(freq, decreasing = TRUE)
freq[head(freq.order)] #most frequent terms
freq[tail(freq.order)] #least frequent terms
# Question: Which group is more interesting, most frequent terms or least frequent terms?
#more
#~~~~~~~~~~~~~~~~~~~~~~
# Find Associated Terms with a specific term, say 'food', with a minimum correlation 0.4
#~~~~~~~~~~~~~~~~~~~~~~~~
findAssocs(tweetDTM, "food", 0.4)
#~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~
# Part 6: Creat a word cloud from tweetCorpus using weighted TFIDF
# export the cloud to a PNG file
#~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~
library(wordcloud)
DTM.tfidf <- DocumentTermMatrix(corpus,
control = list(tokenize=MC_tokenizer, weighting=function(x)
weightTfIdf(x, normalize = TRUE)))
m <- as.matrix(DTM.tfidf)
tf <- sort(colSums(m), decreasing = TRUE)
terms <- names(tf)
cloud <- data.frame(word = terms, freq = tf)
wordcloud(cloud$terms, cloud$freq,
scale=c(6,1),
min.freq = 0.5,
max.words = 30,
random.order= FALSE,
random.color= TRUE,
colors= brewer.pal(8, 'Dark2'))
DTM.tfidf <- DocumentTermMatrix(corpus,
control = list(tokenize=MC_tokenizer, weighting=function(x)
weightTfIdf(x, normalize = TRUE)))
tweets.df <- twListToDF(tweets)
tweets <- searchTwitter("Softball", n=1000, lang="en")
tweets_corpus <- twListToDF(tweets)
#clean the corpus
data <- str_replace_all(tweets_corpus$text,"[^[:graph:]]", " ")
#~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~
#Building a R corpus
#~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~
corpus <- VCorpus(VectorSource(data))
#corpus[[2]][1]
#~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~
#use tm package to clean the corpus
#~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~
corpus <- tm_map(corpus, content_transformer(tolower))
corpus <- tm_map(corpus, removeWords, stopwords("english"))
corpus <- tm_map(corpus, removePunctuation)
corpus <- tm_map(corpus, stripWhitespace)
corpus <- tm_map(corpus, removeWords, stopwords("english"))
#remve the stop words
#the function stopwords reports a list of about 175 words
stopwords()[1:10]
#get the first document in the corpus
#remove the white space again
corpus <- tm_map(corpus, stripWhitespace)
writeLines(as.character(corpus[[2]]))
#~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~
#Bag of Words
#~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~
#split a string on the space: bag.of.words <- strsplit(text, " ")
DTM <- DocumentTermMatrix(corpus)
bag.of.words <- as.vector(DTM$dimnames$Terms)
DTM <- weightTfIdf(DTM, normalize = TRUE)
#~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~
#Stemming using Porter's algorithm
#~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~
corpus.stem <- tm_map(corpus, stemDocument)
DTM.stem <- DocumentTermMatrix(corpus.stem)
bag.of.words.stem <- as.vector(DTM.stem$dimnames$Terms)
#~~~~~~~~~~~~~~~~~~~~~~~~~~~~
#Create a word cloud using weighted tf-idf
#~~~~~~~~~~~~~~~~~~~~~~~~~~~~
DTM.tfidf <- DocumentTermMatrix(corpus,
control = list(tokenize=MC_tokenizer, weighting=function(x)
weightTfIdf(x, normalize = TRUE)))
m <- as.matrix(DTM.tfidf)
tf <- sort(colSums(m), decreasing = TRUE)
terms <- names(tf)
cloud <- data.frame(word = terms, freq = tf)
wordcloud(cloud$terms, cloud$freq,
scale=c(6,1),
min.freq = 0.5,
max.words = 30,
random.order= FALSE,
random.color= TRUE,
colors= brewer.pal(8, 'Dark2'))
wordcloud(cloud$terms, cloud$freq,
scale=c(6,1),
min.freq = 0.5,
max.words = 30,
random.order= FALSE,
random.color= TRUE)
wordcloud(cloud$word, cloud$freq,
scale=c(6,1),
min.freq = 0.5,
max.words = 30,
random.order= FALSE,
random.color= TRUE)
wordcloud(cloud$word, cloud$freq,
scale=c(6,1),
min.freq = 0.5,
max.words = 30,
random.order= FALSE,
random.color= TRUE,
colors= brewer.pal(8, 'Dark2'))
wordcloud(cloud$word, cloud$freq,
scale=c(6,1),
min.freq = 0.5,
max.words = 100,
random.order= FALSE,
random.color= TRUE,
colors= brewer.pal(8, 'Dark2'))
wordcloud(cloud$word, cloud$freq,
scale=c(6,1),
min.freq = 0.5,
random.order= FALSE,
random.color= TRUE,
colors= brewer.pal(8, 'Dark2'))
# sample searchTwitter, you can replace this with other keywords
tweets <- searchTwitter("'Softball' AND 'Win'", n=1000, lang="en")
tweets_corpus <- twListToDF(tweets)
#clean the corpus
data <- str_replace_all(tweets_corpus$text,"[^[:graph:]]", " ")
#~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~
#Building a R corpus
#~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~
corpus <- VCorpus(VectorSource(data))
#corpus[[2]][1]
#~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~
#use tm package to clean the corpus
#~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~
corpus <- tm_map(corpus, content_transformer(tolower))
corpus <- tm_map(corpus, removeWords, stopwords("english"))
corpus <- tm_map(corpus, removePunctuation)
corpus <- tm_map(corpus, stripWhitespace)
corpus <- tm_map(corpus, removeWords, stopwords("english"))
#remve the stop words
#the function stopwords reports a list of about 175 words
stopwords()[1:10]
#get the first document in the corpus
#remove the white space again
corpus <- tm_map(corpus, stripWhitespace)
writeLines(as.character(corpus[[2]]))
#~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~
#Bag of Words
#~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~
#split a string on the space: bag.of.words <- strsplit(text, " ")
DTM <- DocumentTermMatrix(corpus)
bag.of.words <- as.vector(DTM$dimnames$Terms)
DTM <- weightTfIdf(DTM, normalize = TRUE)
#~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~
#Stemming using Porter's algorithm
#~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~
corpus.stem <- tm_map(corpus, stemDocument)
DTM.stem <- DocumentTermMatrix(corpus.stem)
bag.of.words.stem <- as.vector(DTM.stem$dimnames$Terms)
#~~~~~~~~~~~~~~~~~~~~~~~~~~~~
#Create a word cloud using weighted tf-idf
#~~~~~~~~~~~~~~~~~~~~~~~~~~~~
DTM.tfidf <- DocumentTermMatrix(corpus,
control = list(tokenize=MC_tokenizer, weighting=function(x)
weightTfIdf(x, normalize = TRUE)))
m <- as.matrix(DTM.tfidf)
tf <- sort(colSums(m), decreasing = TRUE)
terms <- names(tf)
cloud <- data.frame(word = terms, freq = tf)
wordcloud(cloud$word, cloud$freq,
scale=c(6,1),
min.freq = 0.5,
random.order= FALSE,
random.color= TRUE,
colors= brewer.pal(8, 'Dark2'))
